<!DOCTYPE html>
<html lang="en">
	<head>
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-RENR9BKCTC"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-RENR9BKCTC');
		</script>

		<title>2nd Workshop on Urban Scene Modeling: Where Vision Meets Photogrammetry and Graphics</title>
		<link rel="icon" type="image/x-icon" href="images/logos/favicon.ico">

		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>  
		<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
        <meta name="description" content="CVPR 2025 Workshop on Urban Scene Modeling: Where Vision Meets Photogrammetry and Graphics.">
        <meta name="keywords" content="CVPR, Structured Reconstruction, 3D Reconstruction, CVPR Workshop, Urban Scene Modeling, Photogrammetry, Graphics, NeRF, SfM, SLAM, Gaussian Splatting, Radiance Fields">
        <meta name="author" content="Jack Langerman et al.">
        <meta name="viewport" content="width=device-width, initial-scale=0.75">
        <meta http-equiv="Cache-control" content="public" max-age="86400">

		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
        <link rel="stylesheet" href="css/main.css">
	</head>
	<body>
    <div class="container">

        <header class="row text-center header info">
            <img src="images/background.jpg" alt="Urban Scene Modeling Workshop" class="text-center banner">
            <h1>2nd Workshop on Urban Scene Modeling -- Wed 11 Jun 2025, Full Day. Room 104 D</h1>
            <h3>Where Vision Meets Photogrammetry and Graphics - CVPR 2025</h3>
        </header>
    
        <section id="about">
			<div class="row">
				<div class="col-sm-12 col-md-3">
					<div class="section-header text-left">
						<img src="images/logos/logo-temp.webp" alt="CVPR Workshop Urban Scene Modeling 2025"
							 class="img-responsive" >
					</div>
				</div>
				<div class="col-sm-12 col-md-9">
					<p style="text-align: justify;">
						3D reconstruction and modeling are central to modern computer vision, with rapid 
						urbanization creating urgent social and environmental challenges that demand 
						innovative solutions. While substantial progress has been made in reconstructing 
						basic 3D structures from images and point clouds, the next frontier lies in advancing 
						structured and semantic reconstruction‚Äîmoving beyond low-level 3D information to 
						produce high-level, parametric models that capture both the structure and semantics 
						of human environments.
					</p>
					<p style="text-align: justify;">
						This workshop aims to bridge the gap between state-of-the-art 3D scene modeling and 
						structured, semantic 3D reconstruction by bringing together researchers from 
						photogrammetry, computer vision, generative models, learned representations, and 
						computer graphics. Through invited talks, spotlight presentations, workshop challenges, 
						and a poster session, we will foster interdisciplinary interaction and empower the next 
						generation of 3D reconstruction technologies by integrating techniques from multi-view 
						learning, geometric modeling, and machine perception. We welcome original contributions 
						in structured reconstruction, learning-based approaches, and all areas related to urban 
						scene modeling.
					</p>
				</div>
			</div>
		</section>

        <section id="news">
			<div class="row">
				<h2>News</h2>
				<ul>
					<li>June 4, 2025: Schedule is finalized! üéâ</li>
					<li>March 31, 2025: Notification to Authors! üéâ</li>
					<li>March 24, 2025: Paper submission deadline on CMT! ‚ùó</li>
					<li>Feb 8, 2025: Call for papers online üìù</li>
					<li>Dec 20, 2024: Workshop is accepted for CVPR 2025 üéâ</li>
				</ul>
			</div>
        </section>
		<section id="schedule">
			<div class="row">
				<h2>Workshop Schedule</h2>
				<div class="col-sm-12">
					<ul class="list-unstyled">
						<li><strong>09:00am - 09:10am</strong> ‚Äî Welcome and introduction</li>
						<li><strong>09:10am - 09:50am</strong> ‚Äî Keynote 1: Peter Wonka</li>
						<li><strong>09:50am - 10:30am</strong> ‚Äî Building 3D Challenge (winner talks)</li>
						<li><strong>10:30am - 11:00am</strong> ‚Äî Poster session & Social (coffee break)</li>
						<li><strong>11:00am - 11:40am</strong> ‚Äî Keynote 2: Despoina Paschalidou</li>
						<li><strong>11:40am - 12:20pm</strong> ‚Äî S23DR Challenge (winner talks)</li>
						<li><strong>12:20pm - 13:30pm</strong> ‚Äî Lunch</li>
						<li><strong>13:30pm - 14:10pm</strong> ‚Äî Keynote 3: Daniel G. Aliaga</li>
						<li><strong>14:10pm - 14:50pm</strong> ‚Äî Paper Spotlights
							<ul>
								<small>
								<li><strong>NadirFloorNet: reconstructing multi-room floorplans from a small set of registered panoramic images.</strong> Giovanni Pintore, Uzair Shah, Marco Agus, Enrico Gobbetti</li>
								<li><strong>CityGen: Infinite and Controllable City Layout Generation</strong>. Jie Deng, Wenhao Chai, Jianshu, Guo Qixuan, Huang Junsheng, Huang Wenhao, Hu Shengyu Hao, Jenq-Neng Hwang, Gaoang Wang. <a href="https://arxiv.org/abs/2312.01508">arXiv</a> </li>
								<li><strong>Proc-GS: Procedural Building Generation for City Assembly with 3D Gaussians.</strong> Yixuan Li, Xingjian Ran, Linning Xu, Tao Lu, Mulin Yu, Zhenzhi Wang, Yuanbo Xiangli Dahua Lin, Bo Dai.<a href="https://arxiv.org/abs/2412.07660">arXiv</a> </li>
								<li><strong>Texture2LoD3: Enabling LoD3 Building Reconstruction With Panoramic Images.</strong> Wenzhao Tang, Weihang Li, Xiucheng Liang, Olaf Wysocki, Filip Biljecki, Christoph Holst, Boris Jutzi. <a href="https://arxiv.org/abs/2504.05249">arXiv</a> </li>
								<li><strong>Turin3D: Evaluating Adaptation Strategies under Label Scarcity in Urban LiDAR Segmentation with Semi-Supervised Techniques.</strong> Luca Barco, Giacomo Blanco, Gaetano Chiriaco, Alessia Intini, Luigi La Riccia, Vittorio Scolamiero, Piero Boccardo, Paolo Garza, Fabrizio Dominici. <a href="https://arxiv.org/abs/2504.05882">arXiv.</a></li>
								<li><strong>Semantic 3D Reconstruction from Point Clouds with Sparse Annotations.</strong>  Xin Deng, Bo Yang, Bing Wang.</li>
								<li><strong>Near-incident detection in railroad environments: lateral distance estimation from train-mounted monocular camera.</strong> Yilei Wang, Giacomo D'Amicantonio, Egor Bondarev </li>
							</small>
							</ul>
						</li>
						<li><strong>14:50pm - 15:30pm</strong> ‚Äî Keynote 4: Jonanthan Li</li>
						<li><strong>15:30pm - 16:00pm</strong> ‚Äî Poster session & Social (coffee break)</li>
						<li><strong>16:00pm - 16:40pm</strong> ‚Äî Keynote 5: Federico Tombari</li>
						<li><strong>16:40pm - 17:20pm</strong> ‚Äî Keynote 6: Bj√∂rn Ommer</li>
						<li><strong>17:20pm - 18:00pm</strong> ‚Äî Collaboration and Discussion Session (‚âàinteractive panel)</li>
						<li><strong>18:00pm</strong> ‚Äî Closing Remarks</li>
					</ul>
				</div>
			</div>
		</section>
		
        <section id="speakers">
            <div class="row">
                <h2>Keynote Speakers</h2>

                <div class="col-sm-12">
                    <h3>Daniel G. Aliaga</h3>
                    <img src="images/speakers/2025/daniel.jpg" alt="Daniel G. Aliaga" class="col-sm-12 col-md-3">
                    <h4>Associate Professor of Computer Science, Purdue University</h4>
                    <p>Daniel Aliaga is an Associate Professor in the Department of Computer Science at Purdue University. He holds a Bachelor of Science degree from Brown University and a Ph.D. from the University of North Carolina at Chapel Hill. Prof. Aliaga's research interests encompass urban modeling, reconstruction, and procedural and parametric modeling techniques. He is a pioneer in inverse procedural modeling for urban spaces, aiming to facilitate semi-automatic and controllable content creation and editing of large-scale geometric models. His interdisciplinary work bridges computer science with urban planning, architecture, meteorology, and more, focusing on urban visual computing and artificial intelligence tools to improve urban ecosystems and enable "what-if" exploration of sustainable urban designs. Dr. Aliaga has an extensive publication record and has held visiting positions at institutions such as ETH Zurich, INRIA Sophia-Antipolis, and KAUST.</p>
                </div>
        
                <div class="col-sm-12">
                    <h3>Despoina Paschalidou</h3>
                    <img src="images/speakers/2025/despoina.jpg" alt="Despoina Paschalidou" class="col-sm-12 col-md-3">
                    <h4>Senior Research Scientist, NVIDIA Toronto AI Lab</h4>
                    <p>Despoina Paschalidou is a Senior Research Scientist at the NVIDIA Toronto AI Lab, based in Santa Clara, California. She completed her Ph.D. at the Max Planck ETH Center for Learning Systems, where she was advised by Andreas Geiger and Luc Van Gool. Following her Ph.D., she served as a Postdoctoral Researcher at Stanford University under the supervision of Leonidas Guibas. Dr. Paschalidou's research focuses on developing representations that can reliably perceive, capture, and recreate the 3D world to facilitate seamless human interaction. She has worked on a range of problems, including 3D reconstruction of objects using interpretable primitive-based representations, generative models for objects, scenes, and videos, as well as indoor scene synthesis. Her work aims to advance the state of the art in generative modeling and contribute to the development of technologies that allow for better understanding and interaction with 3D environments.</p>
                </div> 

                <div class="col-sm-12">
                    <h3>Federico Tombari</h3>
                    <img src="images/speakers/2025/federico.jpg" alt="Federico Tombari" class="col-sm-12 col-md-3">
                    <h4>Research Scientist and Manager, Google Zurich; Lecturer, Technical University of Munich (TUM)</h4>
                    <p>Federico Tombari is a Research Scientist and Manager at Google Zurich, where he leads an applied research team in computer vision and machine learning. He is also affiliated with the Faculty of Computer Science at the Technical University of Munich (TUM) as a lecturer (Privatdozent). Dr. Tombari's research focuses on 3D computer vision, including areas such as 3D scene understanding, object recognition, 3D reconstruction and modeling, and simultaneous localization and mapping (SLAM). His work has significant applications in robotics, augmented reality, autonomous driving, and healthcare. He is actively involved in the academic community, serving as an Area Chair for leading conferences like CVPR and NeurIPS, and as an Associate Editor for the International Journal of Robotics Research (IJRR). Dr. Tombari has an extensive publication record and is known for his contributions to the advancement of 3D computer vision.</p>
                </div>

                <div class="col-sm-12">
                    <h3>Jonathan Li</h3>
                    <img src="images/speakers/2025/jonathan.jpg" alt="Jonathan Li" class="col-sm-12 col-md-3">
                    <h4>Professor of Geospatial Data Science, University of Waterloo</h4>
                    <p>Jonathan Li is a full professor at the University of Waterloo, holding appointments in the Department of Geography and Environmental Management and the Department of Systems Design Engineering. His research specializes in urban remote sensing and geospatial data science, with a focus on the automated extraction of geometric and semantic information from Earth observation images and LiDAR point clouds using artificial intelligence algorithms. Prof. Li's recent work involves generating high-definition maps and digital terrain models to support the development of digital twin cities and autonomous vehicles. He is an elected Fellow of several prestigious organizations, including the Institute of Electrical and Electronics Engineers (IEEE), the Royal Society of Canada Academy of Science, the Canadian Academy of Engineering, and the Engineering Institute of Canada. Currently, he serves as the President of the Canadian Institute of Geomatics and is the Editor-in-Chief of the International Journal of Applied Earth Observation and Geoinformation.</p>
                </div>
        
                <div class="col-sm-12">
                    <h3>Peter Wonka</h3>
                    <img src="images/speakers/2025/peter.jpg" alt="Peter Wonka" class="col-sm-12 col-md-3">
                    <h4>Full Professor of Computer Science, King Abdullah University of Science and Technology (KAUST)</h4>
                    <p>Peter Wonka is a Full Professor in the Computer Science Program at King Abdullah University of Science and Technology (KAUST) in Saudi Arabia. His research interests lie in deep learning for visual computing, encompassing computer vision, machine learning, and computer graphics. Prof. Wonka focuses on topics such as generative models, 3D representation learning, geometry processing, and large-scale urban modeling. He is particularly interested in deep learning techniques applied to visual computing tasks, including generative adversarial networks, 3D reconstruction, and neural fields. Prior to joining KAUST, Prof. Wonka held positions at notable institutions and has a strong background in both mathematical and computational aspects of visual computing. He emphasizes impactful research and collaborates closely with students and postdoctoral researchers to advance the field.</p>
                </div>
        
                <div class="col-sm-12">
                    <h3>Bj√∂rn Ommer</h3>
                    <img src="images/speakers/2025/bjorn.jpg" alt="Bj√∂rn Ommer" class="col-sm-12 col-md-3">
                    <h4>Head of Computer Vision & Learning Group, Ludwig Maximilian University of Munich</h4>
                    <p>Bj√∂rn Ommer is a full professor at LMU where he heads the Computer Vision & Learning Group (previously Computer Vision Group Heidelberg). Before he was a full professor at the Department of Mathematics and Computer Science of Heidelberg University and also served as a one of the directors of the Interdisciplinary Center for Scientific Computing (IWR) and of the Heidelberg Collaboratory for Image Processing (HCI). He serves in the Bavarian AI Council, as an associate editor for the journal IEEE T-PAMI, and previously for Pattern Recognition Letters. Bj√∂rn is an ELLIS Fellow, an ELLIS unit faculty of the ELLIS unit Munich, affiliated with the Helmholtz foundation, and a PI of the Munich Center for Machine Learning (MCML). He has served as program chair for GCPR, as Senior Area Chair and Area Chair for multiple CVPR, ICCV, ECCV, and NeurIPS conferences, and as workshop and tutorial organizer at these venues. Bj√∂rn delivered the opening keynote at NeurIPS‚Äô23, was awarded the German AI-Prize 2024, the Technology-Prize of Eduard-Rhein-Foundation 2024, and the work leading to Stable Diffusion has been nominated for the German Future Prize of the President of Germany (‚ÄúDeutscher Zukunftspreis des Bundespr√§sidenten f√ºr Technik und Innovation‚Äù).</p>
                </div>
        
            </div>
        </section>
        <section id="schedule">
    <div class="row">
        <h2>Workshop Schedule</h2>
        <div class="col-sm-12">
            <ul class="list-unstyled">
                <li><strong>09:00am - 09:10am</strong> ‚Äî Welcome and introduction</li>
                <li><strong>09:10am - 09:50am</strong> ‚Äî Keynote 1: Peter Wonka</li>
                <li><strong>09:50am - 10:30am</strong> ‚Äî Building 3D Challenge (winner talks)</li>
                <li><strong>10:30am - 11:00am</strong> ‚Äî Poster session & Social (coffee break)</li>
                <li><strong>11:00am - 11:40am</strong> ‚Äî Keynote 2: Despoina Paschalidou</li>
                <li><strong>11:40am - 12:20pm</strong> ‚Äî S23DR Challenge (winner talks)</li>
                <li><strong>12:20pm - 13:30pm</strong> ‚Äî Lunch</li>
                <li><strong>13:30pm - 14:10pm</strong> ‚Äî Keynote 3: Daniel G. Aliaga</li>
                <li><strong>14:10pm - 14:50pm</strong> ‚Äî Paper Spotlights
                    <ul>
                        <li>NadirFloorNet: reconstructing multi-room floorplans ‚Äî Giovanni Pintore et al.</li>
                        <li>CityGen: Infinite and Controllable City Layout Generation ‚Äî Jie Deng et al.</li>
                        <li>Proc-GS: Procedural Building Generation ‚Äî Yixuan Li et al.</li>
                        <li>Texture2LoD3: LoD3 Building Reconstruction ‚Äî Wenzhao Tang et al.</li>
                        <li>Turin3D: Urban LiDAR Segmentation ‚Äî Luca Barco et al.</li>
                        <li>Semantic 3D Reconstruction ‚Äî Xin Deng et al.</li>
                        <li>Near-incident Detection in Railroads ‚Äî Yilei Wang et al.</li>
                    </ul>
                </li>
                <li><strong>14:50pm - 15:30pm</strong> ‚Äî Keynote 4: Jonanthan Li</li>
                <li><strong>15:30pm - 16:00pm</strong> ‚Äî Poster session & Social (coffee break)</li>
                <li><strong>16:00pm - 16:40pm</strong> ‚Äî Keynote 5: Federico Tombari</li>
                <li><strong>16:40pm - 17:20pm</strong> ‚Äî Keynote 6: Bj√∂rn Ommer</li>
                <li><strong>17:20pm - 18:00pm</strong> ‚Äî Collaboration and Discussion Session (‚âàinteractive panel)</li>
                <li><strong>18:00pm</strong> ‚Äî Closing Remarks</li>
            </ul>
        </div>
    </div>
</section>


        <section id="callforpapers">
			<div class="row">
				<h2>Call for Papers</h2>
                <div class="col-sm-12 col-md-12" >
                    <p>We invite submissions of original research related to urban scene modeling. Topics of interest include, but are not limited to:</p>
                    <ul>
                        <li>Structured 3D Reconstruction/Modeling of human environments, including indoor and outdoor spaces, from sparse, noisy, or partial point clouds and images</li>
                        <li>Semantic, Instance, and Panoptic Segmentation and Parsing of 3D point clouds and images in complex human-centered environments</li>
                        <li>Fusion of Images and Point Clouds to improve the accuracy, detail, and structure of human-centric 3D scene modeling</li>
                        <li>Structured Representation of 3D Scenes, including parametric (e.g., CAD, B-Rep, Wireframe, etc.) models for buildings, interiors, and other human-made structures</li>
                        <li>Neural Implicit Representations for efficient and scalable modeling of large human environments</li>
                        <li>Learning Priors for Structured 3D Modeling, focusing on generating plausible, real-world human environments with structural consistency</li>
                        <li>Generative Models for Occlusion-Free Image Generation, enabling realistic texture mapping and enhanced 3D model quality</li>
                        <li>Multiview 3D Matching and Registration techniques for capturing and integrating scans of complex human spaces</li>
                        <li>Pose Estimation and Structured 3D Recovery from sparse image sets, with applications in architecture, robotics, and other domains</li>
                        <li>Differentiable Rendering and Occlusion Reasoning in human environments, such as indoor spaces, cityscapes, and public infrastructure</li>
                        <li>Applications of Structured and Semantic 3D Reconstruction in smart cities, construction, autonomous navigation, and digital twins</li>
                        <li>Benchmarks and Datasets for large-scale 3D modeling of human environments, driving new challenges and setting the standard for the field</li>
                    </ul>
                    <p>
                    We will accept submissions on two tracks: extended abstract submissions (up to 4 pages) and full papers (up to 8 pages) in the standard CVPR format. Accepted submissions will be presented as posters, and some will be selected for spotlight talks.
                    </p>
                    <p>
                    Paper submission deadline: March 24, 2025. via CMT
                    </p>
                    <p>
                    More details about the submission process will be announced soon.
                    </p>
                </div>
            </div>


			<div class="row">
				<div class="col-sm-12 col-md-12">
					<h3 id="wheretosubmit">Where to Submit</h3>
					<p>All submissions will be handled through the CMT platform and submitted via <a target="_blank" href="http://cmt3.research.microsoft.com/USM3D2025/">this link</a></p>
					
					<a target="_blank" href="http://cmt3.research.microsoft.com/USM3D2025/"><h2>Submit Here</h2></a>

					<h3>How to Submit</h3>
					<p>Authors should submit their papers through CMT. Each submission must include:</p>
					<ul>
						<li>The paper in PDF format following the CVPR format</li>
						<li>Supplementary materials (optional)</li>
						<li>Authors' information including names, affiliations, and contact details</li>
					</ul>
		
					<h3>Submission Guidelines</h3>
					<ul>
						<li>Papers must be submitted in PDF format</li>
						<li>Papers must follow the CVPR format</li>
						<li>Extended abstracts should be up to 4 pages (excluding references)</li>
						<li>Full papers should be up to 8 pages (excluding references)</li>
						<li>Supplementary material is allowed but should be limited to 100MB</li>
						<li>All submissions must be in English</li>
					</ul>
		
					<h3>Author Guidelines</h3>
					<ul>
						<li>At least one author of each accepted paper must register for the workshop and present the paper</li>
						<li>Authors are responsible for ensuring that their submissions do not violate any publication agreements or copyrights</li>
						<li>Double submissions are not allowed</li>
						<li>Papers will be reviewed by at least two expert reviewers</li>
					</ul>
		
					<h3>Important Dates</h3>
					<ul>
						<li><s>Paper submission deadline: March 24, 2025</s></li>
						<li>Notification to authors: March 31, 2025</li>
						<li>Camera-ready deadline: April 7, 2025</li>
					</ul>

					<div>
						<p><strong>Acknowledgment:</strong> The Microsoft CMT service was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.</p>
					</div>
				</div>
			</div>
		</section>


        <section id="challenge">
   
			<div class="row">
				<h2>Challenges</h2>
				<!-- <div class="col-sm-12 col-md-3" style="padding: 0;"> -->
    
                <div>
                    <h3 style="margin-top: 0;">2nd S23DR Challenge</h3>
                    <div>
                        <div class="section-header text-left">
                            <img src="images/s23dr_data.png" alt="S23DR Challenge" style="margin-top: 2em;" >
                        </div>
                        <p>As part of this workshop, we are hosting the
                            <b><a target="_blank" href="https://huggingface.co/spaces/usm3d/S23DR2025">2nd S23DR Challenge</a></b>.
                            The S23DR Challenge focuses on transforming posed images and SfM outputs into structured geometric representations (wire frames) from which semantically meaningful measurements can be extracted. Building on the success of the first challenge, we aim to push the boundaries of structured 3D reconstruction even further.
                            
                            The objective remains the development of More Structured Structure From Motion: the ability to effectively transform posed images and SfM outputs into accurate wire frame models<b>For this second iteration, we introduce new evaluation metrics focused on structural fidelity, along with a 5x larger dataset to enable learning based methods.</b>
                        </p>
                        <h3>Awards & Submissions</h3>
                            <p style="text-align: justify;">
                                The winning submission will receive a cash prize provided by the workshop sponsor and selected
                                finalists will be invited to present their research in the workshop. The prerequisite to receive
                                a money prize including providing a write-up detailing their solution by a submission to the workshop, in our challenge track
                                in the form of <b>an detail experiment log and method explaner, an extended abstract (4 pages), or a full paper (8 pages)</b>, as well as <b>the code required to generate a winning submission under CC BY4.0 license</b>.
                            </p>
                            <p style="text-align: justify;">There is a <b>$ 25,000</b> prize pool for this challenge.</p>
                            <ul>
                                <li>1st Place: <b>$10,000</b></li>
                                <li>2nd Place: <b>$7,000</b></li>
                                <li>3rd Place: <b>$5,000</b></li>
                                <li>Additional Discretionary Prizes: <b>$3,000</b></li>
                            </ul>
                            <p style="text-align: justify;">Please see the Competition Rules under the Rules tab on the <a href="https://huggingface.co/spaces/usm3d/S23DR2025">S23DR Challenge</a> huggingface space for additional information.</p>
                            <p>We thank <a href="https://www.hover.to/">Hover Inc.</a> for their generous sponsorship of this competition.</p>
                            <h3>Important Dates</h3>
                            <p style="text-align: justify;">
                                April 1 2025: &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;Competition starts<br>
                                May 25 2025: &nbsp;&nbsp;&nbsp;&nbsp; Competition ends<br>
                                May 29 2025: &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Notification to Participants<br>
                                June 1 2025: &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Writeup Deadline<br>
                            </p>
                    </div>
                </div>
    
				<div>
                    <h3 style="margin-top: 0;">2nd Building3D Challenge</h3>
                    <div>
                        <div class="section-header text-left">
                            <img src="images/logos/hf.png" alt="Building3D Challenge" style="margin-top: 2em;" >
                        </div>
                        <!-- </div> -->
                        <!-- <div class="col-sm-12 col-md-9"> -->
                        <!-- <h3 style="margin-top: 0;">2nd Building3D Challenge</h3> -->
                        <!-- <p style="text-align: justify;">As part of this workshop, we are hosting the -->
                        <p>As part of this workshop, we are hosting the
                            <b><a target="_blank" href="https://huggingface.co/spaces/Building3D/2ndBuilding3DChallengeCVPR2025USM3DWorkshop">Building3D challenge</a></b>.
                            Building3D is an urban-scale publicly available dataset consisting of more than 160 thousand buildings
                            with corresponding point clouds, meshes, and wireframe models covering 16 cities in Estonia. For this challenge,
                            approximately 36, 000 buildings from the city of Tallinn are used as the training and testing dataset. Among them
                            , we selected 6000 relatively dense and structurally simple buildings as the Entry-level dataset. The wireframe
                            model is composed of points and edges representing shape and outline of the object. We require algorithms to take
                            the original point cloud as input and regress the wireframe model. For the evaluation, the metrics of mean
                            precision and recall are employed to evaluate accuracy of both points and edges, and overall offset of the
                            model is calculated. Additionally, the wireframe edit distance (WED) is used as an additional metric to
                            evaluate the accuracy of generated wireframe models. <b>In contrast to the first Building3D Challenge,
                                a new test dataset with entirely different building styles from the Building3D dataset will be used to evaluate the submissions. Enjoy &#128512;.</b>
                        </p>
                        <h3>Awards & Submissions</h3>
                            <p style="text-align: justify;">
                                The winning submission will receive a cash prize provided by the workshop sponsor and the chosen
                                finalists will be invited to present their research in the workshop. The prerequisite to receive
                                a money prize is to provide a write-up detailing their solution by a submission to the workshop,
                                in the form of <b>an extended abstract (4 pages) or a full paper (8 pages)</b>, as well as <b>the code
                                required to generate a winning submission under CC BY4.0 license</b>.
                            </p>
                            <p style="text-align: justify;">There is a <b>$ 10,000</b> prize pool for this challenge.</p>
                            <ul>
                                <li>1st Place: <b>$5,000</b></li>
                                <li>2nd Place: <b>$3,000</b></li>
                                <li>3rd Place: <b>$1,000</b></li>
                                <li>Additional Prizes: <b>$500</b></li>
                            </ul>
                            <p style="text-align: justify;">Please see the <a href="https://huggingface.co/spaces/Building3D/2ndBuilding3DChallengeCVPR2025USM3DWorkshop">Competition Rules</a> for additional information.</p>
                            <p>We thank gold sponsor <a href="http://www.iskyfly.com/">Intelligence.Ally Technology</a> and
                                silver sponsor <a href="http://www.wucesi.com/">Shenzhen WUCE SPATIAL</a> for their generous sponsorship of this competition.</p>
                            <h3>Important Dates</h3>
                            <p style="text-align: justify;">
                                March 1 2025, Sat.: &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;Competition starts<br>
                                May 25 2025, Sun.: &nbsp;&nbsp;&nbsp;&nbsp; Competition ends<br>
                                May 29 2025,  Thu.: &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Notification to Participants<br>
                                June 1 2025,  Sun.: &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Writeup Deadline<br>
                            </p>
                    </div>
                </div>
			</div>
        </section>

        <section id="organizers">
            <h2>Organizers</h2>
            <div class="row text-center">
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://profiles.ucalgary.ca/ruisheng-wang">
                        <img src="images/organizers/ruisheng-wang.jpg" alt="Ruisheng Wang" class="img-circle headshot">
                        <span class="name">Ruisheng <br> Wang</span>
                        <span class="affiliation">Professor <br> Shenzhen University</span>
                    </a>
                </div>
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://jackml.com">
                        <img src="images/organizers/jack-langerman.png" alt="Jack Langerman" class="img-circle headshot">
                        <span class="name">Jack <br> Langerman</span>
                        <span class="affiliation">Applied Researcher <br> Apple</span>
                    </a>
                </div>
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://ilkedemir.weebly.com">
                        <img src="images/organizers/ilke-demir.png" alt="Ilke Demir" class="img-circle headshot">
                        <span class="name">Ilke <br> Demir</span>
                        <span class="affiliation">Senior Research Scientist <br> Intel Corporation</span>
                    </a>
                </div>
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://dmytro.ai">
                        <img src="https://dmytro.ai/photo.jpg" alt="Dmytro Mishkin" class="img-circle headshot">
                        <span class="name">Dmytro <br> Mishkin</span>
                        <span class="affiliation">Researcher <br> Czech Technical University in Prague</span>
                    </a>
                </div>
            </div>
            <div class="row text-center">
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://www.imperial.ac.uk/people/t.birdal">
                        <img src="images/organizers/tolga-birdal.jpg" alt="Tolga Birdal" class="img-circle headshot">
                        <span class="name">Tolga <br> Birdal</span>
                        <span class="affiliation">Assistant Professor <br> Imperial College London</span>
                    </a>
                </div>
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://saup.szu.edu.cn/info/1091/1415.htm">
                        <img src="images/organizers/renzhong-guo.jpg" alt="Renzhong Guo" class="img-circle headshot">
                        <span class="name">Renzhong <br> Guo</span>
                        <span class="affiliation">Professor <br> Shenzhen University</span>
                    </a>
                </div>
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://shangfenghuang.github.io">
                        <img src="images/organizers/shangfeng-huang.png" alt="Shangfeng Huang" class="img-circle headshot">
                        <span class="name">Shangfeng <br> Huang</span>
                        <span class="affiliation">Researcher <br> University of Calgary</span>
                    </a>
                </div>
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://www.linkedin.com/in/seanxiangma/">
                        <img src="images/organizers/xiang-ma.png" alt="Xiang Ma" class="img-circle headshot">
                        <span class="name">Xiang <br> Ma</span>
                        <span class="affiliation">Head of Research <br> Amazon Web Services</span>
                    </a>
                </div>
            </div>
            <div class="row text-center">
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://www.umr-lastig.fr/clement-mallet/">
                        <img src="images/organizers/clement-mallet.jpeg" alt="Clement Mallet" class="img-circle headshot">
                        <span class="name">Clement <br> Mallet</span>
                        <span class="affiliation">Research Scientist <br> LASTIG</span>
                    </a>
                </div>
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://users.encs.concordia.ca/~wayang/">
                        <img src="images/organizers/yang-wang.jpeg" alt="Yang Wang" class="img-circle headshot">
                        <span class="name">Yang <br> Wang</span>
                        <span class="affiliation">Associate Professor <br> Concordia University</span>
                    </a>
                </div>
                <div class="col-xs-6 col-sm-3">
                    <a target="_blank" href="https://www.yuzhonghuang.org/">
                        <img src="images/organizers/yuzhong-huang.jpeg" alt="Yuzhong Huang" class="img-circle headshot">
                        <span class="name">Yuzhong <br> Huang</span>
                        <span class="affiliation">Researcher <br>University of Southern California</span>
                    </a>
                </div>
            </div>
    </div>
    </section>

    <section id="links" class="container">
        <h2>Links</h2>
        <ul>
            <li><p>Previous websites: <a href="./2024/index.html">2024</a></p></li>
            <li><p>This is a <a href="http://cvpr2025.thecvf.com/">CVPR 2025</a> workshop</p></li>
        </ul>
    </section>

    <footer class="text-center footer">
        <p>All Rights Reserved. &copy; 2025</p>
    </footer>

    </div>

</body>
</html>
